{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b429b0-c7d8-4306-9948-9ec77d5b7014",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 4 is due on 3/3/25 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83931375-25f0-4da0-b1a2-4a5f6ee4b34c",
   "metadata": {},
   "source": [
    "Create a dataset D which we call `Xy` such that the linear model has  R² about 0\\% but x, y are clearly associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c04981-d8f7-4300-a5bd-a8faf57e25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from plotnine import ggplot, aes, geom_point, scale_y_continuous, labs, ggtitle\n",
    "\n",
    "# Set numpy to ignore overflow warnings\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "# Create x values from 1 to 200\n",
    "x = np.arange(1, 201, dtype=np.float64)\n",
    "\n",
    "# Compute y as x raised to the 67th power\n",
    "y = x ** 67\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Fit the linear model: y ~ x\n",
    "X = sm.add_constant(df['x'])  # Adds an intercept\n",
    "model = sm.OLS(df['y'], X).fit()\n",
    "print(\"R-squared:\", model.rsquared)\n",
    "\n",
    "# Plot using plotnine with specified y-axis limits\n",
    "p = (\n",
    "    ggplot(df, aes(x='x', y='y'))\n",
    "    + geom_point()\n",
    "    + scale_y_continuous(limits=(-7.37e152, 20))\n",
    "    + labs(x='x', y='y')\n",
    "    + ggtitle('Scatter Plot of x vs y')\n",
    ")\n",
    "\n",
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5dded3-dc80-490a-acde-5de7ed6e44fc",
   "metadata": {},
   "source": [
    "The issue is that when you compute y = x⁶⁷ on x values from 1 to 200, the resulting numbers are extremely large. Even when computed as floats, this can lead to numerical instability in the regression calculation, causing R² to become NaN. One common fix is to scale down y before fitting the model. Run the code below and see the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebfd7c-f242-4999-a789-5f80473f251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from plotnine import ggplot, aes, geom_point, scale_y_continuous, labs, ggtitle\n",
    "\n",
    "# Set numpy to ignore overflow warnings during exponentiation\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "# Create x values as floats\n",
    "x = np.arange(1, 201, dtype=np.float64)\n",
    "\n",
    "# Compute y as x raised to the 67th power\n",
    "y = x ** 67\n",
    "\n",
    "# Scale down y for regression purposes (R² is invariant under scaling)\n",
    "scaling_factor = 1e154\n",
    "y_scaled = y / scaling_factor\n",
    "\n",
    "# Build the DataFrame with original and scaled y values\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'y_scaled': y_scaled})\n",
    "\n",
    "# Fit the linear model on the scaled y\n",
    "X = sm.add_constant(df['x'])\n",
    "model = sm.OLS(df['y_scaled'], X).fit()\n",
    "print(\"R-squared:\", model.rsquared)\n",
    "\n",
    "# Define the y-axis limits as Python floats\n",
    "y_min = float(-7.37e152)\n",
    "y_max = float(1.54e154)\n",
    "\n",
    "# Provide explicit breaks and labels to avoid automatic tick calculation issues\n",
    "p = (\n",
    "    ggplot(df, aes(x='x', y='y'))\n",
    "    + geom_point()\n",
    "    + scale_y_continuous(\n",
    "          limits=(y_min, y_max),\n",
    "          breaks=[y_min, 0.0, y_max],\n",
    "          labels=[f\"{y_min:.2e}\", \"0\", f\"{y_max:.2e}\"]\n",
    "      )\n",
    "    + labs(x='x', y='y')\n",
    "    + ggtitle('Scatter Plot of x vs y')\n",
    ")\n",
    "\n",
    "print(p)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bdddf-49b4-4604-9f82-f737fcde046e",
   "metadata": {},
   "source": [
    "Write a function `my_ols` that takes in `X`, a matrix with with p columns representing the feature measurements for each of the n units, a vector of n responses `y` and returns a list that contains the `b`, the p+1-sized column vector of OLS coefficients, `yhat` (the vector of n predictions), `e` (the vector of n residuals), `df` for degrees of freedom of the model, `SSE`, `SST`, `MSE`, `RMSE` and `Rsq` (for the  R² metric). You will have to use `linalg.inv` from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d43b2-d6b0-496d-af8f-15ea6b85a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom class to mimic R's class assignment\n",
    "class MyOLSResult(dict):\n",
    "    pass\n",
    "\n",
    "def my_ols(X, y, add_intercept=True):\n",
    "    # Convert inputs to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Check that X is numeric\n",
    "    if not np.issubdtype(X.dtype, np.number):\n",
    "        raise ValueError(\"X is not numeric\")\n",
    "    # Check that y is numeric\n",
    "    if not np.issubdtype(y.dtype, np.number):\n",
    "        raise ValueError(\"y needs to be numeric\")\n",
    "    # Check that the number of rows in X equals the length of y\n",
    "    if X.shape[0] != len(y):\n",
    "        raise ValueError(\"X rows and length of y need to be the same length.\")\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    # Add an intercept column if requested\n",
    "    if add_intercept:\n",
    "        X = np.column_stack((np.ones(n), X))\n",
    "    \n",
    "    p = X.shape[1]  # p is the number of parameters (including the intercept if added)\n",
    "    df = n - p      # residual degrees of freedom\n",
    "    \n",
    "    if n <= p:\n",
    "        raise ValueError(\"There must be more observations than parameters\")\n",
    "    \n",
    "    y_bar = np.mean(y)\n",
    "    \n",
    "    # Compute OLS coefficients: b = (X^T X)^{-1} X^T y\n",
    "    XtX = X.T @ X\n",
    "    b = np.linalg.inv(XtX) @ (X.T @ y)\n",
    "    \n",
    "    # Compute predictions and residuals\n",
    "    yhat = X @ b\n",
    "    e = y - yhat\n",
    "    \n",
    "    # Compute SSE and SST\n",
    "    SSE = e.T @ e\n",
    "    SST = np.sum((y - y_bar)**2)\n",
    "    \n",
    "    # Compute MSE, RMSE, and R-squared\n",
    "    MSE = SSE / (n - p)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    Rsq = 1 - (SSE / SST)\n",
    "    \n",
    "    # Construct the result as an instance of MyOLSResult\n",
    "    result = MyOLSResult({\n",
    "        'b': b,\n",
    "        'yhat': yhat,\n",
    "        'df': df,\n",
    "        'e': e,\n",
    "        'SSE': SSE,\n",
    "        'SST': SST,\n",
    "        'MSE': MSE,\n",
    "        'RMSE': RMSE,\n",
    "        'Rsq': Rsq,\n",
    "        'p': p\n",
    "    })\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f35ac-5afa-4660-8958-31332f22d2d0",
   "metadata": {},
   "source": [
    "Verify that the OLS coefficients for the `Type` of cars in the cars dataset gives you the same results as we did in class (i.e. the ybar's within group). You will load the cars data set by importing data from pydataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9467ab-83fc-4196-b489-0586674e2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "\n",
    "# Load the Cars93 dataset (equivalent to MASS::Cars93)\n",
    "\n",
    "\n",
    "# Create a design matrix for Type with no intercept,\n",
    "# similar to model.matrix(~ 0 + Type, data = cars) in R.\n",
    "\n",
    "\n",
    "# Get the Price vector\n",
    "\n",
    "\n",
    "# Run our custom OLS function with add_intercept set to False\n",
    "result = my_ols()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f6607-1051-4192-a49e-fef20ea08b07",
   "metadata": {},
   "source": [
    "Create a prediction method `g` that takes in a vector `x_star` and the dataset D i.e. `X` and `y` and returns the OLS predictions. Let `X` be a matrix with with p columns representing the feature measurements for each of the n units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948ad4b-06b1-47fd-a2a8-ffc572b0f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g(x_star, X, y, add_intercept=True):\n",
    "    \"\"\"\n",
    "    Prediction function that returns the OLS prediction for a new observation.\n",
    "    \n",
    "    Parameters:\n",
    "        x_star : array-like\n",
    "            A vector of feature values for the new observation.\n",
    "        X : array-like\n",
    "            The design matrix for the training data.\n",
    "        y : array-like\n",
    "            The response vector for the training data.\n",
    "        add_intercept : bool (default True)\n",
    "            Whether the my_ols function should add an intercept.\n",
    "    \n",
    "    Returns:\n",
    "        The OLS prediction for x_star.\n",
    "    \"\"\"\n",
    "    # Get the OLS result from our custom function\n",
    "    \n",
    "    \n",
    "    # Ensure x_star is a numpy array\n",
    " \n",
    "    \n",
    "    # If an intercept was added in the model, prepend a 1 to x_star.\n",
    "    \n",
    "    # Return the dot product (prediction)\n",
    "    \n",
    "\n",
    "# Create a design matrix for Type with no intercept, and force numeric conversion\n",
    "X = pd.get_dummies(cars['Type'], prefix='Type', drop_first=False).astype(float)\n",
    "y = cars['Price'].to_numpy()\n",
    "\n",
    "# Convert X to a NumPy array so that indexing works as expected\n",
    "X_np = X.to_numpy()\n",
    "\n",
    "# Now, predict for the first observation in X using our custom function g\n",
    "prediction = g()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a77898-0482-4595-b2dc-402d6ca505cf",
   "metadata": {},
   "source": [
    "Load up the famous iris dataset. We are going to do a different prediction problem. Imagine the only input x is Species and you are trying to predict y which is Petal.Length. A reasonable prediction is the average petal length within each Species. Prove that this is the OLS model by fitting an appropriate `ols` and then using the `mod.predict` function to verify. Note: this time we will load the iris dataset from another python package called seaborn which we will cover in dept during lab 6 when we go over visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721efc8-56b6-4aaa-bedc-d8cc330612a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e913e93-8ed4-46f8-8158-e54535c6451e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the iris dataset from seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Optionally, rename columns to match the R names (Petal.Length becomes Petal_Length and Species stays Species)\n",
    "iris.rename(columns={\"petal_length\": \"Petal_Length\", \"species\": \"Species\"}, inplace=True)\n",
    "\n",
    "# Fit the OLS model: Petal_Length ~ Species.\n",
    "# This will create dummy variables internally and use one species as the baseline.\n",
    "mod = smf.ols().fit()\n",
    "\n",
    "# Compute the mean Petal_Length within each Species\n",
    "mean_setosa = iris.loc[iris[\"Species\"] == \"setosa\", \"Petal_Length\"].mean()\n",
    "\n",
    "\n",
    "print(\"Mean Petal_Length for setosa: \", mean_setosa)\n",
    "\n",
    "\n",
    "# Use predict to obtain predictions for each Species.\n",
    "pred_setosa = mod.predict(pd.DataFrame({\"Species\": [\"setosa\"]}))\n",
    "\n",
    "\n",
    "print(\"Predicted Petal_Length for setosa: \", pred_setosa.iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727937e-0626-4be9-8475-ebf85b9712f0",
   "metadata": {},
   "source": [
    "You may have noticed that to get the OLS function from statsmodel we used smf instead of sm. Either method can be used but the\n",
    "difference is in the interface they provide:\n",
    "\n",
    "sm (statsmodels.api):\n",
    "This is the lower-level API where you work directly with data arrays or matrices. You have to manually create the design matrix (including adding an intercept if needed), and then pass it to functions like sm.OLS. This interface is similar to using raw matrix algebra.\n",
    "\n",
    "smf (statsmodels.formula.api):\n",
    "This offers a higher-level, formula-based interface (similar to R's lm), where you can specify your model using a formula string (e.g., \"y ~ x\") and a DataFrame. The function automatically constructs the design matrix and handles categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209ab1e-f547-429a-af9f-089778f04f35",
   "metadata": {},
   "source": [
    "Construct the design matrix with an intercept, X manually. You will need to use np.column_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f3ee6-aa8c-4567-a2c2-d214b3332e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the design matrix manually:\n",
    "# - The first column is an intercept (all ones)\n",
    "# - The second column is 1 if species is \"versicolor\", else 0\n",
    "# - The third column is 1 if species is \"virginica\", else 0\n",
    "X = np.column_stack((\n",
    "    np.ones(),\n",
    "    (iris['Species'] == \"versicolor\").astype(int),\n",
    "    ()\n",
    "))\n",
    "\n",
    "# Show the first few rows of the design matrix\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d59949-99f8-4b37-878b-cfe6ff6a9e1d",
   "metadata": {},
   "source": [
    "We now import the diamonds dataset from plotnine.data. In R you Skim the dataset using skimr or summary. Here we will use .info() and/or .describe from pandas. What is the datatype of the color feature? (hint: use .dtype on the color feature from the dataframe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d7e40-e172-409e-a759-2443139cb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine.data import diamonds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519eb395-7278-4132-ad1f-ad302a838b2b",
   "metadata": {},
   "source": [
    "Find the levels of the color feature. Using pandas this can be accomplished by assigning the feature \"color\" to a variable by using .cat.categories from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9dfaa9-9f83-46f3-ac14-e4120bd1551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the levels (categories) of the 'color' feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c04329-aac2-4097-8e5d-95c1967941d9",
   "metadata": {},
   "source": [
    "Create new feature in the diamonds dataset, `color_as_numeric`, which is color expressed as a continuous interval value. Use .cat.codes from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134360a-f65b-4ec3-8983-a844ba99073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature 'color_as_numeric' with 1-based coding (like R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d98aa-c626-4639-ae3c-0b6ccc49b2e1",
   "metadata": {},
   "source": [
    "Use that converted feature as the one predictor in a regression. How well does this regression do as measured by RMSE? Use sm from statsmodels.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad59af-6804-43b6-8399-3d0672db1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21d9f5-e7b5-4d20-9064-8d50fc9bd80a",
   "metadata": {},
   "source": [
    "Try this again using smf from statsmodels.api and see if the result matches the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a8ff3-dfb1-4440-b93d-c24cf844cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c59ccf-06a3-49b1-94ca-f61d3fe3f545",
   "metadata": {},
   "source": [
    "Create new feature in the diamonds dataset, `color_as_nominal`, which is color expressed as a nominal categorical variable. Use .Categorial and set ordered = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3b8ce-3274-4b2f-bb39-f041de42ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature 'color_as_nominal' as a nominal (unordered) categorical variable\n",
    "\n",
    "\n",
    "#Creating the Nominal Feature:\n",
    "#We use pd.Categorical to convert the \"color\" column into a categorical variable. \n",
    "#By setting ordered=False, we ensure it is treated as nominal (unordered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3eda4-bd0f-46d6-bc89-90fd901612be",
   "metadata": {},
   "source": [
    "Use that converted feature as the one predictor in a regression. How well does this regression do as measured by RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c09928-ead2-4e51-9d98-91dc3b70ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f36a576-ea45-42b8-8e0e-353fbbc12741",
   "metadata": {},
   "source": [
    "Which regression does better - `color_as_numeric` or `color_as_nominal`? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fe8b7-eb69-48a3-a195-68c5abdf5ef5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fa2bb7-548f-4b4d-a6e5-92a037364bc6",
   "metadata": {},
   "source": [
    "Now regress both `color_as_numeric` and `color_as_nominal` in a regression. Does this regression do any better (as gauged by RMSE) than either color_as_numeric` or `color_as_nominal` alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cca97-3f47-4467-ab8b-20a4c2586f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model using both predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da900a-8b5f-46fe-824a-63876d505832",
   "metadata": {},
   "source": [
    "What are the coefficients (the b vector)? You can access the parameters using .params and assigning it to a variable `b` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf12501-4873-4a8a-804e-1b05ee67fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient vector (b vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b04ac7-e03f-42bd-a866-d5e4035de663",
   "metadata": {},
   "source": [
    "Your Python results should be very similar to what you'd expect in R. The slight differences can be attributed to a few factors:\n",
    "\n",
    "Factor Level Ordering:\n",
    "R and Python (via pandas) may assign the underlying integer codes to factor levels in slightly different orders depending on the default level ordering. This can affect the dummy coding and, as a result, the estimated coefficients.\n",
    "\n",
    "Numerical Precision and Implementation:\n",
    "The linear algebra routines in R and Python (via NumPy/statsmodels) may differ slightly in numerical precision or the way they handle floating‐point arithmetic. These differences can lead to small discrepancies in coefficient estimates.\n",
    "\n",
    "Contrast Coding Differences:\n",
    "R’s default dummy coding (treatment contrasts) and statsmodels’ handling of categorical variables (which also uses treatment coding by default) are conceptually similar but might differ in subtle details (e.g., which level is chosen as the baseline).\n",
    "\n",
    "Overall, the estimates are close enough that these differences are within the range expected from using different software implementations for the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56963716-386c-477b-affa-abb780abd670",
   "metadata": {},
   "source": [
    "Something appears to be anomalous in the coefficients. What is it? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1574831-653b-4865-97dc-2ba34db02ad1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bffa6bc-575a-4b0e-b23b-db0704ca1af0",
   "metadata": {},
   "source": [
    "Return to the iris dataset. Find the hat matrix H for this regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962eb2a-44a1-445e-93fe-4dae9dd88da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, matrix_rank\n",
    "\n",
    "# Construct the design matrix X:\n",
    "# 1 for the intercept, then indicators for Species == \"versicolor\" and Species == \"virginica\"\n",
    "X = np.column_stack((\n",
    "   \n",
    "))\n",
    "\n",
    "# Compute the hat matrix H = X (X^T X)^{-1} X^T\n",
    "H = \n",
    "\n",
    "# Compute the rank of H\n",
    "rank_H = matrix_rank()\n",
    "print(\"Rank of H:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d68257-0d51-49ff-b919-a52a83ae2b82",
   "metadata": {},
   "source": [
    "Verify this hat matrix is symmetric using the `assert_allclose` from `npt` from numpy.testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21410fa4-9742-43a0-bf66-128eb0191a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214caba-bf3e-4a67-830b-3b757dc30de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    print(\"Hat matrix is symmetric.\")\n",
    "except AssertionError as e:\n",
    "    print(\"Hat matrix is not symmetric:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042a30f-d2ca-4c70-8803-7044361e8d4e",
   "metadata": {},
   "source": [
    "Verify this hat matrix is idempotent using the `assert_allclose` from `npt` from numpy.testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99686c2b-829f-48b4-a9c4-3ca491bbe746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6c7c2-80df-4807-a6c8-b3dff4487ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "   \n",
    "    print(\"Hat matrix .\")\n",
    "except AssertionError as e:\n",
    "    print(\"Hat matrix :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4444ea2-9d10-47f7-b917-50f029e3c831",
   "metadata": {},
   "source": [
    "Using the `np.diag` function from numpy, find the trace of the hat matrix (i.e. the sum of the diagonal elements in the matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6addc-07b7-480e-a190-1803f8a929cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given H is your hat matrix\n",
    "\n",
    "print(\"Trace of H:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d9fce-ee9d-4054-a8ab-ae99421f350a",
   "metadata": {},
   "source": [
    "It turns out the trace of a hat matrix is the same as its rank! But we don't have time to prove these interesting and useful facts..\n",
    "\n",
    "Extra Credit (+5): create a matrix X-perpendicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d845a6-2442-4a6e-93ff-69ab1c1eaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume H and X have been defined previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441cb27-46dd-464d-afb7-2c4715793c5d",
   "metadata": {},
   "source": [
    "Using the hat matrix, compute the yhat vector and using the projection onto the residual space, compute the e vector and verify they are orthogonal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef92daf-2afc-4f2f-8ca0-0703efdc541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response vector: equivalent to iris$Petal.Length in R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce985075-0abe-49b9-b051-6c1770f9d53f",
   "metadata": {},
   "source": [
    "Compute SST, SSR and SSE and R² and then show that SST = SSR + SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5bfb1-6b9e-4139-a001-fb3dfd207a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SSE: sum of squared errors\n",
    "SSE = \n",
    "\n",
    "# Compute y_bar: mean of y\n",
    "y_bar = \n",
    "\n",
    "# Compute SST: total sum of squares\n",
    "SST = \n",
    "\n",
    "# Compute SSR: regression sum of squares\n",
    "SSR = \n",
    "\n",
    "print(\"SSR:\", )\n",
    "print(\"SSE:\", )\n",
    "print(\"SST:\", )\n",
    "\n",
    "# Verify that SST equals SSR + SSE within a tolerance\n",
    "\n",
    "print(\"Verified: SST equals SSR + SSE.\")\n",
    "\n",
    "# Compute R² as SSR / SST\n",
    "R_squared = SSR / SST\n",
    "print(\"R²:\", R_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63c3f0-4e32-4937-b6b1-47afccc46f0d",
   "metadata": {},
   "source": [
    "Find the angle theta between y - ybar 1 and yhat - ybar 1 and then verify that its cosine squared is the same as the R² from the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac01c3-1299-4f5f-a9a1-c93c7bbdf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product of (y - y_bar) and (y_hat - y_bar)\n",
    "\n",
    "# Compute the denominator: sqrt(SST * SSR)\n",
    "\n",
    "\n",
    "# Compute theta in radians using arccos\n",
    "\n",
    "\n",
    "# Convert theta to degrees\n",
    "\n",
    "\n",
    "print(\"Theta (in radians):\", )\n",
    "print(\"Theta (in degrees):\", )\n",
    "\n",
    "# Verify that cos^2(theta) is equal to R²\n",
    "\n",
    "print(\"cos^2(theta):\", )\n",
    "print(\"R²:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345db190-1e95-46ef-96bd-feedc659cfa3",
   "metadata": {},
   "source": [
    "Project the y vector onto each column of the X matrix and test if the sum of these projections is the same as yhat. Use `assert_allclose` from numpy (This should fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c59de-a9f4-416b-869e-11c7f8d55adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection onto the first column of X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9846b7-f94b-4ecc-b836-a880d3b56adc",
   "metadata": {},
   "source": [
    "Construct the design matrix without an intercept, X, without using pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6226c7-2aa2-43f8-8b2f-d104b6e2dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the design matrix without an intercept:\n",
    "# Each column is an indicator (dummy variable) for one species.\n",
    "X = pd.DataFrame({\n",
    "    \n",
    "})\n",
    "\n",
    "# The response vector is petal_length.\n",
    "\n",
    "\n",
    "# Show the first few rows of the design matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367da097-23d1-41f7-bdb3-75ae252625e2",
   "metadata": {},
   "source": [
    "Find the OLS estimates using this design matrix. It should be the sample averages of the petal lengths within species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f4959-68ac-4cac-90a1-dadc55e3626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2041e9b6-1bad-4dd7-9bf2-8aff19b25769",
   "metadata": {},
   "source": [
    "Verify the hat matrix constructed from this design matrix is the same as the hat matrix constructed from the design matrix with the intercept by using `assert_allclose`. (Fact: orthogonal projection matrices are unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2926e6e-f295-478b-93cd-d2712d70af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the two hat matrices are equal within numerical tolerance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd72d2-1d72-411b-a81f-f4c8a08b26f1",
   "metadata": {},
   "source": [
    "Project the y vector onto each column of the X matrix and test if the sum of these projections is the same as yhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923efc9-5288-4620-a891-ca810da2444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming H2 and y have already been computed (from previous steps),\n",
    "# and yhatnew was computed as H2 @ y.\n",
    "\n",
    "\n",
    "# Verify that the projection of y via H2 equals yhatnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b942ef-81d1-437c-87f4-c21a24ff115d",
   "metadata": {},
   "source": [
    "Convert this design matrix into Q, an orthonormal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f46d97-b003-4b69-b746-d2870c93eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the QR decomposition of x. Q is the orthonormal matrix.\n",
    "Q, R = np.linalg.qr(x) #tuple unpacking to assign Q (the orthonormal matrix) and R(the upper triangular matrix)\n",
    "\n",
    "# Check normalization: Each column of Q should have unit length.\n",
    "print(\"Sum of squares of Q[:,0]:\", np.sum(Q[:, 0]**2))\n",
    "print(\"Sum of squares of Q[:,1]:\", np.sum(Q[:, 1]**2))\n",
    "\n",
    "\n",
    "# Check orthogonality: Dot products between different columns should be 0.\n",
    "print(\"Dot product Q[:,0] and Q[:,1]:\", np.dot(Q[:, 0], Q[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e797c3a-93f3-4377-a571-4923b9065ba9",
   "metadata": {},
   "source": [
    "Project the y vector onto each column of the Q matrix and test if the sum of these projections is the same as yhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912b6fe-ef28-466d-9385-259a08dece99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the projection onto each column of Q.\n",
    "# Since Q is orthonormal, Q[:,i].T @ Q[:,i] == 1, so:\n",
    "proj1 = (Q[:, 0:1] @ Q[:, 0:1].T) @ y\n",
    "\n",
    "\n",
    "# Sum the individual projections.\n",
    "yhatQ = \n",
    "\n",
    "# Alternatively, since Q is orthonormal, we can compute:\n",
    "yhatQ_direct = Q @ Q.T @ y\n",
    "\n",
    "# Verify that yhatQ equals yhatnew.\n",
    "npt.assert_allclose(yhatQ, yhatnew, rtol=1e-7, atol=1e-10)\n",
    "npt.assert_allclose(yhatQ_direct, yhatnew, rtol=1e-7, atol=1e-10)\n",
    "print(\"The projection using Q equals yhatnew.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fb153-411b-4b0c-a66f-1427fc56b44b",
   "metadata": {},
   "source": [
    "Find the p=3 linear OLS estimates if Q is used as the design matrix using the `sm.OLS` from statsmodels. Is the OLS solution the same as the OLS solution for X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0565b-cb53-4a88-b2fe-112571e7a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the design matrix x without an intercept:\n",
    "# Each column is an indicator for one species.\n",
    "x = np.column_stack((\n",
    "    (iris['Species'] == \"setosa\").astype(int),\n",
    "    (iris['Species'] == \"versicolor\").astype(int),\n",
    "    (iris['Species'] == \"virginica\").astype(int)\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e4c9c-da4c-408e-bf11-c6dc280b1b91",
   "metadata": {},
   "source": [
    "Use the predict function and ensure that the predicted values are the same for both linear models: the one created with X  as its design matrix and the one created with Q as its design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdc0cd-2513-471f-a312-7caf502159df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column names to the design matrix (no intercept)\n",
    "col_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "# Create DataFrames for x and Q\n",
    "\n",
    "\n",
    "# Fit the OLS models with no intercept (since x and Q already have no intercept column)\n",
    "mod3 = sm.OLS(y, df_x).fit()\n",
    "\n",
    "\n",
    "# Use the predict method to get the fitted values\n",
    "pred_mod3 = mod3.predict(df_x)\n",
    "\n",
    "\n",
    "# Get the unique predicted values (rounded for numerical stability)\n",
    "\n",
    "\n",
    "print(\"Unique predictions from mod3 (using x):\", )\n",
    "print(\"Unique predictions from mod4 (using Q):\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b92b3-be42-4270-8e07-944edef278c4",
   "metadata": {},
   "source": [
    "Load the boston housing data and extract X and y. The dimensions are n = 506 and p = 13. Create a matrix that is (p + 1) x (p + 1) full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the y regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the y regressed on the first and second columns of X only and put them in the first and second entries. For the third row, find the OLS estimates of the y regressed on the first, second and third columns of X only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643e8ca-8a58-448f-86c2-9db8e0e44f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston housing data from the provided URL\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "# The dataset is stored in two alternating rows.\n",
    "# The first row of each pair contains predictor values,\n",
    "# the second row contains the response (target) as well as extra predictors.\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Number of observations\n",
    "n = data.shape[0]\n",
    "\n",
    "# Construct the design matrix X with an intercept:\n",
    "# First column is ones, then the 13 predictors.\n",
    "X = np.hstack()\n",
    "y = target\n",
    "\n",
    "# Create column names: \"Intercept\" plus the 13 predictor names.\n",
    "# (Using the standard Boston dataset names)\n",
    "col_names = np.concatenate(([\"Intercept\"], \n",
    "                            [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \n",
    "                             \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]))\n",
    "\n",
    "# Create a (p+1)x(p+1) matrix (14x14) filled with np.nan\n",
    "M = np.full((X.shape[1], ), )\n",
    "\n",
    "# For each i from 1 to 14, compute the OLS estimates using the first i columns of X.\n",
    "for i in range(1, X.shape[1] + 1):\n",
    "    b = np.full(X.shape[1], np.nan)  # Temporary vector of length 14 filled with np.nan\n",
    "    X2 = X[:, :i]  # Use the first i columns of X\n",
    "    # Compute OLS estimate: beta = (X2^T X2)^{-1} X2^T y\n",
    "    beta = np.linalg.inv(X2.T @ X2) @ (X2.T @ y)\n",
    "    b[:i] = beta  # Place the estimates in the first i entries\n",
    "    M[i-1, :] = b\n",
    "\n",
    "# Convert M to a DataFrame and label its columns\n",
    "M_df = pd.DataFrame()\n",
    "print(M_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce5a12-6217-4f6d-aefa-3173e6e6b056",
   "metadata": {},
   "source": [
    "Why are the estimates changing from row to row as you add in more predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e09a4-03c6-4eb6-af3c-d0b03b75675a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3d21c5-e5c5-4195-a03c-124638d652a4",
   "metadata": {},
   "source": [
    "Create a vector of length p+1 and compute the R² values for each of the above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e057024-710e-434d-823e-b44c0a8cfeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ybar and SST\n",
    "ybar = \n",
    "SST =\n",
    "\n",
    "# Initialize an array to hold the R² values for each model (there are 14 models)\n",
    "Rsq = np.empty()\n",
    "\n",
    "# Loop over i = 1 to 14 (each row of M corresponds to a model with the first i predictors)\n",
    "p_plus_1 = M.shape[0]  # This should be 14\n",
    "for i in range(1, p_plus_1 + 1):\n",
    "    # Create a coefficient vector of length 14.\n",
    "    # Use the first i coefficients from M (row i-1) and pad the rest with zeros.\n",
    "    b = np.concatenate((M[i-1, :i], np.zeros(p_plus_1 - i)))\n",
    "    \n",
    "    # Compute the fitted values for the full design matrix X using these coefficients\n",
    "    yhat = X @ b\n",
    "    \n",
    "    # Compute the regression sum of squares (SSR)\n",
    "    SSR = \n",
    "    \n",
    "    # Compute R² as SSR / SST\n",
    "    Rsq[i-1] = \n",
    "\n",
    "print(\"R² values for each model:\")\n",
    "print(Rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78da86-2a25-4ee7-97f9-8612e262cc58",
   "metadata": {},
   "source": [
    "Is R² monotonically increasing? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543a900-f7e5-4777-b96d-daae87f87576",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9a736ca-a9b9-4ab1-b003-59bec77c0ea6",
   "metadata": {},
   "source": [
    "Create a 2x2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns in absolute difference from 90 degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246741c-369b-4916-8eb3-2556b215d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(v):\n",
    "    return np.sqrt(np.sum(v**2))\n",
    "\n",
    "# Create a 2x2 matrix: first column is all ones; second column is iid N(0,1)\n",
    "X = np.ones()\n",
    "X[:, 1] = np.\n",
    "\n",
    "# Compute the cosine of the angle between the first and second column\n",
    "cos_theta = np.dot(X[:, 0], X[:, 1]) / (norm_vec(X[:, 0]) * norm_vec(X[:, 1]))\n",
    "print(\"Cosine of the angle:\", cos_theta)\n",
    "\n",
    "# Compute the angle in degrees\n",
    "angle_degrees = np.arccos(cos_theta) * 180 / np.pi\n",
    "print(\"Angle between columns (degrees):\", angle_degrees)\n",
    "\n",
    "# Compute the absolute difference from 90 degrees\n",
    "abs_diff = \n",
    "print(\"Absolute difference from 90 degrees:\", abs_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf082c7-1334-47a6-b2ca-a8fa36b0235d",
   "metadata": {},
   "source": [
    "Repeat this exercise `Nsim = 1e5` times and report the average absolute angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3c27d-50e7-4cc0-bdc7-015b5ac74f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsim = int(1e5)\n",
    "angles = np.empty(Nsim)\n",
    "\n",
    "for i in range(Nsim):\n",
    "    # Construct a 2x2 matrix: first column is all ones, second column is two iid normals.\n",
    "    X = np.ones((2, 2))\n",
    "    X[:, 1] = np.random.randn(2)\n",
    "    \n",
    "    # Compute cosine of the angle between the two columns.\n",
    "    cos_theta = np.dot(X[:, 0], X[:, 1]) / (norm_vec(X[:, 0]) * norm_vec(X[:, 1]))\n",
    "    \n",
    "    # Compute the angle in degrees.\n",
    "    angle = np.arccos(cos_theta) * 180 / np.pi\n",
    "    \n",
    "    # Compute the absolute difference from 90 degrees.\n",
    "    angles[i] = abs(90 - angle)\n",
    "\n",
    "# Report the average absolute angle difference.\n",
    "avg_angle = np.mean(angles)\n",
    "print(\"Average absolute angle difference (degrees):\", avg_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5e0ad-cde9-42f7-ba02-0479f124086e",
   "metadata": {},
   "source": [
    "Create a n x 2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns. For n = 10, 50, 100, 200, 500, 1000, report the average absolute angle over `Nsim = 1e5` simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cedcc2-658f-4f2d-a841-e4d37be1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(v):\n",
    "    return np.linalg.norm(v)\n",
    "\n",
    "# Define sample sizes (n values) as in R code\n",
    "N_s = np.array()\n",
    "Nsim = int(1e5)\n",
    "\n",
    "# Create an empty array to store the angle differences\n",
    "angles = np.empty((Nsim, len(N_s)))\n",
    "\n",
    "# Loop over each sample size\n",
    "for j, n_val in enumerate(N_s):\n",
    "    for i in range(Nsim):\n",
    "        # Create a n_val x 2 matrix: first column ones, second column iid normals.\n",
    "      \n",
    "        \n",
    "        # Compute cosine of the angle between the two columns.\n",
    "        # The first column is all ones, so its norm is sqrt(n_val).\n",
    "       \n",
    "        \n",
    "        # Compute the angle in degrees\n",
    "        \n",
    "        \n",
    "        # Compute the absolute difference from 90 degrees\n",
    "     \n",
    "\n",
    "# Compute the average absolute angle for each sample size\n",
    "avg_angles = np.mean(angles, axis=0)\n",
    "print(\"Average absolute angle differences (in degrees) for each n in N_s:\")\n",
    "print(avg_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec0dde-c92f-47cf-af1b-7c3ccd934ece",
   "metadata": {},
   "source": [
    "What is this absolute angle difference from 90 degrees converging to? Why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b56a5-1c6b-461d-bc56-ef54fc9f442c",
   "metadata": {},
   "source": [
    "As the sample size grows, the constant column (all ones) becomes orthogonal to the random normal column (which averages to nearly 0), and so the angle between them becomes 90°, making the absolute difference from 90° shrink t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
